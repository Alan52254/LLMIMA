# -*- coding: utf-8 -*-
"""LLMIMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sl8vg-wS7KV5BfotqWYl2j-JPrF0_5ql
"""

!pip install requests groq python-dotenv

!pip install fastapi

# 導入必要的模組
from dotenv import load_dotenv
import os

# 指定 llm.env 的路徑
dotenv_path = "/content/llm.env"  # 如果在 Colab 中，改成 "/content/llm.env"

# 載入環境變數
load_dotenv(dotenv_path)

# 檢查是否成功載入
print("載入結果:", load_dotenv(dotenv_path))  # 應輸出 True

# 獲取並顯示 API key
api_key = os.getenv("GROQ_API_KEY")
print("GROQ_API_KEY:", api_key)  # 應輸出你的 API key

# 如果載入失敗，直接設置（備用方案）
if not api_key:
    os.environ["GROQ_API_KEY"] = "your_api_key"  # 替換成真實 key
    print("直接設置後:", os.getenv("GROQ_API_KEY"))

# 後續使用 api_key，例如呼叫某個 API
# 例如：some_api_call(api_key)

import os
os.environ["GROQ_API_KEY"] = "your_api_key"

import json
import os
import requests
from typing import Dict, Any
from fastapi import HTTPException
from groq import Groq
from dotenv import load_dotenv

# ------------------------------------------------------------------------------
# Load environment variables and initialize clients
# ------------------------------------------------------------------------------

# Load environment variables from .env file
load_dotenv()

# Retrieve API keys and Prometheus URL from environment variables
groq_api_key = os.getenv("GROQ_API_KEY")
PROMETHEUS_URL = os.getenv("PROMETHEUS_URL", "Your_PROMETHEUS_URL")

# Ensure required API keys are provided
if not groq_api_key:
    raise ValueError("GROQ_API_KEY environment variable is not set")
if not PROMETHEUS_URL:
    raise ValueError("PROMETHEUS_URL environment variable is not set")

# Initialize Groq client and specify model (using available text model)
client = Groq(api_key=groq_api_key)
model = "llama3-70b-8192"  # 推薦使用文字模型

# ------------------------------------------------------------------------------
# Prometheus API Functions
# ------------------------------------------------------------------------------

def get_pod_cpu_usage(namespace: str, pod: str, range_str: str = "[1h]", aggregation: str = "sum") -> str:
    """
    查詢指定 Namespace 和 Pod 的 CPU 使用量。

    Args:
        namespace (str): Kubernetes namespace.
        pod (str): Pod name.
        range_str (str): Time range for the query (e.g., "[1h]", "[1w]").
        aggregation (str): Aggregation method (sum, avg, max, min).

    Returns:
        str: JSON string of CPU usage data.
    """
    metric_name = "container_cpu_usage_seconds_total"
    query = f'{aggregation}(rate({metric_name}{{namespace="{namespace}", pod="{pod}"}}[5m])) {range_str}'
    try:
        resp = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={"query": query},
            timeout=10
        )
        resp.raise_for_status()
        data = resp.json()
        if data.get("status") == "success":
            return json.dumps(data["data"])
        else:
            return json.dumps({"error": f"Prometheus query failed: {data.get('error')}"})
    except requests.exceptions.RequestException as e:
        return json.dumps({"error": f"Prometheus API request failed: {str(e)}"})

def get_pod_memory_usage(namespace: str, pod: str, range_str: str = "[1h]", aggregation: str = "sum") -> str:
    """
    查詢指定 Namespace 和 Pod 的記憶體使用量。

    Args:
        namespace (str): Kubernetes namespace.
        pod (str): Pod name.
        range_str (str): Time range for the query (e.g., "[1h]", "[1w]").
        aggregation (str): Aggregation method (sum, avg, max, min).

    Returns:
        str: JSON string of memory usage data.
    """
    metric_name = "container_memory_usage_bytes"
    query = f'{aggregation}({metric_name}{{namespace="{namespace}", pod="{pod}"}}) {range_str}'
    try:
        resp = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={"query": query},
            timeout=10
        )
        resp.raise_for_status()
        data = resp.json()
        if data.get("status") == "success":
            return json.dumps(data["data"])
        else:
            return json.dumps({"error": f"Prometheus query failed: {data.get('error')}"})
    except requests.exceptions.RequestException as e:
        return json.dumps({"error": f"Prometheus API request failed: {str(e)}"})

def get_node_cpu_usage(node: str, range_str: str = "[1h]") -> str:
    """
    查詢指定節點的 CPU 使用量。

    Args:
        node (str): Node name.
        range_str (str): Time range for the query (e.g., "[1h]", "[1w]").

    Returns:
        str: JSON string of CPU usage data.
    """
    metric_name = "node_cpu_seconds_total"
    query = f'sum(rate({metric_name}{{instance="{node}"}}[5m])) by (mode) {range_str}'
    try:
        resp = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={"query": query},
            timeout=10
        )
        resp.raise_for_status()
        data = resp.json()
        if data.get("status") == "success":
            return json.dumps(data["data"])
        else:
            return json.dumps({"error": f"Prometheus query failed: {data.get('error')}"})
    except requests.exceptions.RequestException as e:
        return json.dumps({"error": f"Prometheus API request failed: {str(e)}"})

def get_node_memory_usage(node: str, range_str: str = "[1h]") -> str:
    """
    查詢指定節點的記憶體使用量。

    Args:
        node (str): Node name.
        range_str (str): Time range for the query (e.g., "[1h]", "[1w]").

    Returns:
        str: JSON string of memory usage data.
    """
    metric_name = "node_memory_MemTotal_bytes"
    query = f'{metric_name}{{instance="{node}"}} - node_memory_MemAvailable_bytes{{instance="{node}"}} {range_str}'
    try:
        resp = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={"query": query},
            timeout=10
        )
        resp.raise_for_status()
        data = resp.json()
        if data.get("status") == "success":
            return json.dumps(data["data"])
        else:
            return json.dumps({"error": f"Prometheus query failed: {data.get('error')}"})
    except requests.exceptions.RequestException as e:
        return json.dumps({"error": f"Prometheus API request failed: {str(e)}"})

def get_all_nodes_metrics(range_str: str = "[1h]") -> str:
    """
    查詢所有節點的 CPU 和記憶體使用量。

    Args:
        range_str (str): Time range for the query (e.g., "[1h]", "[1w]").

    Returns:
        str: JSON string containing metrics for all nodes.
    """
    try:
        # Get CPU metrics
        cpu_query = f'sum(rate(node_cpu_seconds_total[5m])) by (instance) {range_str}'
        cpu_resp = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={"query": cpu_query},
            timeout=10
        )
        cpu_resp.raise_for_status()
        cpu_data = cpu_resp.json()

        # Get memory metrics
        memory_query = f'node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes {range_str}'
        memory_resp = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={"query": memory_query},
            timeout=10
        )
        memory_resp.raise_for_status()
        memory_data = memory_resp.json()

        # Combine results
        combined_data = {
            "cpu": cpu_data.get("data", {}),
            "memory": memory_data.get("data", {})
        }
        return json.dumps(combined_data)
    except requests.exceptions.RequestException as e:
        return json.dumps({"error": f"Prometheus API request failed: {str(e)}"})

def get_top_cpu_pods(namespace: str, k: int = 3) -> str:
    """
    查詢指定 Namespace 下，前 K 個 CPU 用量最高的 Pods。

    Args:
        namespace (str): Kubernetes namespace.
        k (int): Number of top pods to return (default: 3).

    Returns:
        str: JSON string of top CPU pods data.
    """
    query = f'topk({k}, sum by (pod) (rate(container_cpu_usage_seconds_total{{namespace="{namespace}"}}[5m])))'
    try:
        resp = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={"query": query},
            timeout=10
        )
        resp.raise_for_status()
        data = resp.json()
        if data.get("status") == "success":
            return json.dumps(data["data"])
        else:
            return json.dumps({"error": f"Prometheus query failed: {data.get('error')}"})
    except requests.exceptions.RequestException as e:
        return json.dumps({"error": f"Prometheus API request failed: {str(e)}"})

def get_top_memory_pods(namespace: str, k: int = 3) -> str:
    """
    查詢指定 Namespace 下，前 K 個記憶體用量最高的 Pods。

    Args:
        namespace (str): Kubernetes namespace.
        k (int): Number of top pods to return (default: 3).

    Returns:
        str: JSON string of top memory pods data.
    """
    query = f'topk({k}, sum by (pod) (container_memory_usage_bytes{{namespace="{namespace}"}}))'
    try:
        resp = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={"query": query},
            timeout=10
        )
        resp.raise_for_status()
        data = resp.json()
        if data.get("status") == "success":
            return json.dumps(data["data"])
        else:
            return json.dumps({"error": f"Prometheus query failed: {data.get('error')}"})
    except requests.exceptions.RequestException as e:
        return json.dumps({"error": f"Prometheus API request failed: {str(e)}"})

# ------------------------------------------------------------------------------
# Groq Chat API Helper Functions
# ------------------------------------------------------------------------------

def get_model_response(messages: list, tools: list) -> dict:
    """
    Call the Groq chat API with the given messages and tools, then return the response.

    Args:
        messages (list): The conversation history.
        tools (list): A list of tool definitions available to the assistant.

    Returns:
        dict: Processed assistant response including content and any tool calls.
    """
    response_obj = client.chat.completions.create(
        model=model,
        messages=messages,
        tools=tools,
        tool_choice="auto",
        max_tokens=1024,  # 這邊建議不要太多先測
        stream=False
    )
    message = response_obj.choices[0].message
    return {
        "role": "assistant",
        "content": message.content,
        "tool_calls": getattr(message, "tool_calls", [])
    }

def process_tool_calls(response_message: dict, messages: list, available_functions: dict):
    """
    Process any tool calls from the assistant's response.

    Args:
        response_message (dict): The assistant's response containing potential tool calls.
        messages (list): The conversation history to be updated with tool responses.
        available_functions (dict): Mapping from tool names to actual functions.
    """
    for tool_call in response_message.get("tool_calls", []):
        function_name = tool_call.function.name
        function_to_call = available_functions.get(function_name)
        if function_to_call:
            function_args = json.loads(tool_call.function.arguments)
            function_response = function_to_call(**function_args)
            messages.append({
                "role": "tool",
                "content": str(function_response),
                "tool_call_id": tool_call.id,
            })

# ------------------------------------------------------------------------------
# Main Prometheus Metrics Flow
# ------------------------------------------------------------------------------

def get_prometheus_metrics():
    """
    Orchestrates the process of querying Prometheus metrics using LLM.

    1. Gets a user query about Kubernetes metrics.
    2. Sets up initial conversation messages and tool definitions.
    3. Makes calls to the Groq model to process the query.
    4. Processes tool calls (e.g., fetching CPU/memory usage) and updates the conversation.
    """
    # Get the metrics query from the user
    metrics_query = input("Enter your Prometheus metrics query (e.g., 'Get CPU usage for pod my-pod in namespace default over the last week'): ")

    # Define the initial conversation context for the assistant
    messages = [
        {
            "role": "system",
            "content": (
                "You are a Kubernetes monitoring assistant. Help the user fetch Prometheus metrics for their Kubernetes cluster. "
                "Use the provided tools to query CPU and memory usage for specific pods, nodes, or get aggregated metrics across the cluster. "
                "You can query metrics over different time ranges (e.g., [1h], [1d], [1w]) and use different aggregation methods (sum, avg, max, min). "
                "If the user asks for a specific pod's metrics, use get_pod_cpu_usage or get_pod_memory_usage with the appropriate namespace, pod name, time range, and aggregation method. "
                "If the user asks for node metrics, use get_node_cpu_usage or get_node_memory_usage. "
                "For cluster-wide metrics, use get_all_nodes_metrics. "
                "If the user asks for top pods, use get_top_cpu_pods or get_top_memory_pods with the namespace and k value. "
                "Present the results in a clear, organized way, summarizing the data if necessary."
            )
        },
        {"role": "user", "content": metrics_query}
    ]

    # Define tool functions available to the assistant
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_pod_cpu_usage",
                "description": "Get CPU usage for a specific pod in a namespace over a time range with optional aggregation",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "namespace": {
                            "type": "string",
                            "description": "Kubernetes namespace of the pod",
                        },
                        "pod": {
                            "type": "string",
                            "description": "Name of the pod",
                        },
                        "range_str": {
                            "type": "string",
                            "description": "Time range for the query (e.g., '[1h]', '[1d]', '[1w]')",
                            "default": "[1h]"
                        },
                        "aggregation": {
                            "type": "string",
                            "description": "Aggregation method (sum, avg, max, min)",
                            "default": "sum"
                        }
                    },
                    "required": ["namespace", "pod"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_pod_memory_usage",
                "description": "Get memory usage for a specific pod in a namespace over a time range with optional aggregation",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "namespace": {
                            "type": "string",
                            "description": "Kubernetes namespace of the pod",
                        },
                        "pod": {
                            "type": "string",
                            "description": "Name of the pod",
                        },
                        "range_str": {
                            "type": "string",
                            "description": "Time range for the query (e.g., '[1h]', '[1d]', '[1w]')",
                            "default": "[1h]"
                        },
                        "aggregation": {
                            "type": "string",
                            "description": "Aggregation method (sum, avg, max, min)",
                            "default": "sum"
                        }
                    },
                    "required": ["namespace", "pod"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_node_cpu_usage",
                "description": "Get CPU usage for a specific node over a time range",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "node": {
                            "type": "string",
                            "description": "Name of the node",
                        },
                        "range_str": {
                            "type": "string",
                            "description": "Time range for the query (e.g., '[1h]', '[1d]', '[1w]')",
                            "default": "[1h]"
                        }
                    },
                    "required": ["node"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_node_memory_usage",
                "description": "Get memory usage for a specific node over a time range",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "node": {
                            "type": "string",
                            "description": "Name of the node",
                        },
                        "range_str": {
                            "type": "string",
                            "description": "Time range for the query (e.g., '[1h]', '[1d]', '[1w]')",
                            "default": "[1h]"
                        }
                    },
                    "required": ["node"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_all_nodes_metrics",
                "description": "Get CPU and memory usage metrics for all nodes over a time range",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "range_str": {
                            "type": "string",
                            "description": "Time range for the query (e.g., '[1h]', '[1d]', '[1w]')",
                            "default": "[1h]"
                        }
                    },
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_top_cpu_pods",
                "description": "Get the top K pods by CPU usage in a namespace",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "namespace": {
                            "type": "string",
                            "description": "Kubernetes namespace to query",
                        },
                        "k": {
                            "type": "integer",
                            "description": "Number of top pods to return",
                            "default": 3
                        }
                    },
                    "required": ["namespace"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_top_memory_pods",
                "description": "Get the top K pods by memory usage in a namespace",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "namespace": {
                            "type": "string",
                            "description": "Kubernetes namespace to query",
                        },
                        "k": {
                            "type": "integer",
                            "description": "Number of top pods to return",
                            "default": 3
                        }
                    },
                    "required": ["namespace"],
                },
            },
        }
    ]

    # Mapping of available function names to their actual implementations
    available_functions = {
        "get_pod_cpu_usage": get_pod_cpu_usage,
        "get_pod_memory_usage": get_pod_memory_usage,
        "get_node_cpu_usage": get_node_cpu_usage,
        "get_node_memory_usage": get_node_memory_usage,
        "get_all_nodes_metrics": get_all_nodes_metrics,
        "get_top_cpu_pods": get_top_cpu_pods,
        "get_top_memory_pods": get_top_memory_pods,
    }

    # --------------------------
    # First Assistant Call: Process the user's query
    # --------------------------
    initial_response = get_model_response(messages, tools)
    messages.append(initial_response)

    # Process any tool calls from the initial response
    process_tool_calls(initial_response, messages, available_functions)

    # --------------------------
    # Second Assistant Call: Incorporate tool results into conversation
    # --------------------------
    final_response = get_model_response(messages, tools)
    messages.append(final_response)

    # Process any additional tool calls if needed
    if final_response.get("tool_calls"):
        process_tool_calls(final_response, messages, available_functions)
        # Final call to incorporate the tool response results
        final_response = get_model_response(messages, tools)

    # Optional: Print out the final response content
    print("Assistant response:", final_response.get("content"))

# ------------------------------------------------------------------------------
# Main Program Start
# ------------------------------------------------------------------------------
if __name__ == "__main__":
    get_prometheus_metrics()

#我把可以用的模型放在下方
import requests

api_key = ""
headers = {"Authorization": f"Bearer {api_key}"}
url = "https://api.groq.com/openai/v1/models"

resp = requests.get(url, headers=headers)
print(resp.json())

import requests

PROMETHEUS_URL = "Your_Prometheus_URL"# 替換為你的 ngrok URL
try:
    response = requests.get(f"{PROMETHEUS_URL}/api/v1/query?query=up")
    print("Prometheus 連線測試:", response.json())
except Exception as e:
    print("Prometheus 連線失敗:", str(e))